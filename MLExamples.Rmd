---
title: "Machine Learning Techniques"
author: "Graeham Rieman"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
      depth: 2
---

```{r setup}
library(magrittr)
library(ggplot2)
library(feather)
library(plotly)


load_mnist <- function() {
  load_image_file <- function(filename) {
    ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
  }
  load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
  }
  train <<- load_image_file('mnist/train-images.idx3-ubyte')
  test <<- load_image_file('mnist/t10k-images.idx3-ubyte')
  
  train$y <<- load_label_file('mnist/train-labels.idx1-ubyte')
  test$y <<- load_label_file('mnist/t10k-labels.idx1-ubyte')  
}

show_digit <- function(arr784, col=gray(12:1/12), ...) {
  image(matrix(unlist(arr784), nrow=28)[,28:1], col=col, ...)
}

load_mnist()

trainset <- as.data.frame(train$x);trainset$y <- train$y
testset <- as.data.frame(test$x);testset$y <- test$y

trainset %<>% subset(y %in% c("0","8","2","7"))
testset %<>% subset(y %in% c("0","8","2","7"))

write_feather(trainset, "trainset.feather")
write_feather(trainset, "testset.feather")
```

# Background

* Principal Component Analysis is a method of multivariate (multidimensional) data analysis
* It is used for data exploration and dimensionality reduction. Other applications can include clustering and variable selection.
* Care should be taking when interpreting Principal Component Analysis. Diagnostics are available to understand the model output
* Statistical inference is possible from PCA but it may require the use of other techniques, such as KNN to complement PCA and form a conclusion from the simplification of the data
* Please do not call this method PrinciPLE Component Analysis. That is wrong

# Basic Theory
* PCA aims to capture the essence of the multivariate data
* It provides a high level picture of the entire dataset in two or three "Principal Components" (PCs)
* It relies on the covariance between all variables to extract the essence of the data. It uses matrix algebra to calculate eigenvalues and eigenvectors from covariance matrix
* Eigenvectors contain one "weight" for each variable. A linear equation can be formed to using these weights and the value of the variable for each record to calculate an observation's PC1 value.
* There is one Principal Component for each variable used. 
+ The first principal component (PC1) accounts for most of the data's variation as possible
+ Subsequent PCs accounts for the largest possible amount of variance while being orthagonal to the previous PCs
+ Not all PCs should be used to interpret the data

![](Images/PCA.png)



Mathematically, PC1 is written as $w_{(1)}$ and solved for using this equation:

\begin{equation}
w_{(1)} = \text{arg max} \frac{w^T X^T X w}{w^T w} = \text{eigenvector}_{(1)} \text{of} X^T X
\end{equation}

# Sample application of PCA

## Data

### NIMST dataset
We use a subset of the [MNIST handwritten digits dataset](http://yann.lecun.com/exdb/mnist/) to demonstrate the concept of PCA. The NIMST dataset contains "pictures" of handwritten digits from 0 to 9. The "picture" for each of these numbers is stored in a 28 x 28 matrix containing a total of 784 pixels. Each "pixel" is a black/white value with a range of 0 to 255. When plotted in a heatmap style, the darker/light contrast reveals the picture of the number. The zero from the NIMST dataset (labelled as a `r trainset$y[1]`) looks like this:

```{r}
show_digit(trainset[1, 1:784])
```

### Subset of NIMST dataset
For this example we subset the original NIMST data to contain only pictures of digits 1, 4 and 9. The 28x28 matrices for each of these digits is concatenated vertically, resulting in a 84x28 matrix. A 29th column is a label for the number, but it is not used in the analysis. 

#### PCA Analysis using R

PCA Analysis is conducted and the Eigen Values and Eigen Vectors are generated. (Still need to include them below). A "scree plot" shows the percent of the variance explained by each Principal Component (28 in total). It indicates how much you should trust any one component, based on how variance they explain. The higher order PCs usually tend to explain noise in the data so it is not as valuable to interpret them. Below is a picture of the scree plot. Within the scree plot the goal is to find the "elbow" which is usually a good indication that the PCs up to that point explain most of the variance in the data. In this case, three PCs seems to explain the data well, although they collectively only explain about 30% of the data. 

```{r, PCA r}
#Data has been loaded above

#function: prcomp from the stats package. The stats package is installed by default
#  since all our values have the same scale, we will not scale and recenter; this could be done 
#  by setting prcomp(train$x, center=TRUE, scale.=TRUE)
unlabeled <- trainset[, 1:784]
modpca <- prcomp(unlabeled)
prop.pca <- modpca$sdev^2/sum(modpca$sdev^2)
plot(prop.pca[1:28])
```

<!-- #### Python -->

<!-- ```{python, PCA python} -->
<!-- import feather -->
<!-- import pandas as pd -->
<!-- from sklearn.decomposition import RandomizedPCA -->
<!-- import seaborn as sns -->

<!-- #load the data that was loaded then saved in R -->
<!-- train = feather.read_dataframe("E:\\MLExamples\\trainset.feather") -->
<!-- lbls = train['y'] -->
<!-- imgs = train.drop('y', 1) -->

<!-- # train model -->
<!-- pca = RandomizedPCA(n_components=2)  -->
<!-- X = pca.fit_transform(imgs) -->

<!-- # form DataFrame and rename columns -->
<!-- X = pd.DataFrame(X) -->
<!-- X.columns = ['PC1', 'PC2'] -->
<!-- X['class'] = lbls -->

<!-- # make and save plot -->
<!-- sns.set_palette(sns.color_palette("Set3", 10)) -->
<!-- sns.lmplot('PC1', 'PC2', data=X, hue='class', fit_reg=False).savefig('E:\\MLExamples\\python_PCA.png') -->
<!-- ``` -->


### Results

Each PC explains some essence in the data. As seen from the eigenvectors above and the plots below show what is captured by the principal components (only PC1 through PC4 below). Each captures a different "story" and they only account for `r prop.pca[1:3]`, and `r prop.pca[4]` percent of the variation in the data. Althought they are not precisely recognizable as numbers there appear to be groups of curves that make up the digits. PC1 seems to show data that looks like a (nine), while PC2 is a little more unclear in that it shows data that could be a 9, a 4 or a 1. PC3 shows data that could be a 9. 

```{r, PCs}
#plot eigenvectors
par(mfrow=c(2,2))
show_digit(modpca$rotation[,1],axes=FALSE)
show_digit(modpca$rotation[,2],axes=FALSE)
show_digit(modpca$rotation[,3],axes=FALSE)
show_digit(modpca$rotation[,4],axes=FALSE)
par(mfrow=c(1,1))
```

Since each observation is made up of a combination of these Principle Components, we would expect the zeros to have high PC1 values, which we see is true by this plot below, with PC1 on the x-axis and PC2 on the y-axis. However, most of the classes are hard to separate.

```{r, PCA plot}
#ggplot2::ggplot(data=as.data.frame(modpca$x[,1:2]), ggplot2::aes(x=PC1, y=PC2, color=as.factor(trainset$y))) +
#        ggplot2::geom_point(size=2) + RRhelpr::theme_RR() + scale_color_brewer(type="qual", palette="Set3") +
#        ggplot2::labs(x = paste("PC1 (", scales::percent(prop.pca[1]), " explained var.)", sep=""),
#                      y = paste("PC2 (", scales::percent(prop.pca[2]), " explained var.)", sep=""))


plot_ly(as.data.frame(modpca$x), x=~PC1, y=~PC2, z=~PC3, color = as.factor(trainset$y), type = 'scatter3d')
```

Similarly, this is the plot that the above python code generated:

![](Images/python_PCA.png)



## LDA

### Theory
### Use Cases
### Examples


# SVM

# KNN

# Bayesian

# Trees

# Neural Networks

# HMM

# Ensemble Techniques

## Random Forest
## Bagging & Boosting

# Unsupervised Learning