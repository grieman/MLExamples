---
title: "Machine Learning Techniques"
author: "Graeham Rieman"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
      depth: 2
---

```{r setup}
library(magrittr)
library(ggplot2)
library(feather)
library(plotly)

load_mnist <- function() {
  load_image_file <- function(filename) {
    ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
  }
  load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
  }
  train <<- load_image_file('mnist/train-images.idx3-ubyte')
  test <<- load_image_file('mnist/t10k-images.idx3-ubyte')
  
  train$y <<- load_label_file('mnist/train-labels.idx1-ubyte')
  test$y <<- load_label_file('mnist/t10k-labels.idx1-ubyte')  
}

show_digit <- function(arr784, col=gray(12:1/12), ...) {
  image(matrix(unlist(arr784), nrow=28)[,28:1], col=col, ...)
}

load_mnist()

trainset <- as.data.frame(train$x);trainset$y <- as.factor(train$y)
testset <- as.data.frame(test$x);testset$y <- as.factor(test$y)

trainset %<>% subset(y %in% c("1","4","9")) %>% droplevels
testset %<>% subset(y %in% c("1","4","9")) %>% droplevels

write_feather(trainset, "trainset.feather")
write_feather(testset, "testset.feather")
```

# Data

For this example, we will use the [MNIST handwritten digits dataset](http://yann.lecun.com/exdb/mnist/) as it has already been prepared and offers an interesting, commonly used classification problem. Each digit is a 28x28 pixelated image, stored as a vector of 784 black/white values ranging from 0 to 255. This also allows us to more easily visualize each data point - having each column correspond to part of an image rather than to the readout of a sensor allows for a simpler understanding of the classes. Everyone has an idea of what _four_ looks like, even though there are many ways to write it. To save time and processing power, we will only use the digits 0, 1, 7, and 8. The first digit in the train set is labelled as a `r trainset$y[1]`, and looks like this:

```{r}
show_digit(trainset[1, 1:784])
```

# Dimensionality Reductions

These methods aren't quite classifiers, but they do simplify the dataset and make it easier to model properly. They may or may not be able to make predictions on their own - often they require some other algorithm, such as K-Nearest Neighbors, to go from a simplification to a conclusion. 

## Principal Component Analysis


### Theory
Principal Component Analysis (PCA) is a transformation algorithm that attempts to find the Principal Components (PC) of a data set, such that the first principal component (PC1) accounts for as much of the data's variation as possible, and that each subsequent PC accounts for the largest possible amount of variance while being orthagonal to the previous PCs.

![](Images/PCA.png)

Mathematically, PC1 is written as $w_{(1)}$ and solved for using this equation:

\begin{equation}
w_{(1)} = \text{arg max} \frac{w^T X^T X w}{w^T w} = \text{eigenvector}_{(1)} \text{of} X^T X
\end{equation}

### Use Cases
This method may be best used when trying to analyze multi-dimensional data, as it can greatly reduce the dimensionality and keep seperation between classes. It can also highlight which variables are most important, based on their weights in the PC's.

### Examples

#### R

```{r, PCA r}
#Data has been loaded above

#function: prcomp from the stats package. The stats package is installed by default
#  since all our values have the same scale, we will not scale and recenter; this could be done 
#  by setting prcomp(train$x, center=TRUE, scale.=TRUE)
unlabeled <- trainset[, 1:784]
modpca <- prcomp(unlabeled)
prop.pca <- modpca$sdev^2/sum(modpca$sdev^2)
```

#### Python

```{python, PCA python}
import feather, os
import pandas as pd
from sklearn.decomposition import RandomizedPCA
import seaborn as sns

path = os.getcwd()
trainpath = os.path.join(path, "trainset.feather")
plotpath = os.path.join(path, "python_PCA.png")

#load the data that was loaded then saved in R
train = feather.read_dataframe(trainpath)
lbls = train['y']
imgs = train.drop('y', 1)

# train model
pca = RandomizedPCA(n_components=2) 
X = pca.fit_transform(imgs)

# form DataFrame and rename columns
X = pd.DataFrame(X)
X.columns = ['PC1', 'PC2']
X['class'] = lbls

# make and save plot
sns.set_palette(sns.color_palette("Set3", 10))
sns.lmplot('PC1', 'PC2', data=X, hue='class', fit_reg=False).savefig(plotpath)
```


### Results

These four plots show the first four principal components - as noted in the plot above, they only account for `r prop.pca[1:3]`, and `r prop.pca[4]` percent of the variation in the data. Thus they aren't easily recognizable as numbers, but appear to be groups of curves that make up the digits. 

```{r, PCs}
#plot eigenvectors
par(mfrow=c(2,2))
show_digit(modpca$rotation[,1],axes=FALSE)
show_digit(modpca$rotation[,2],axes=FALSE)
show_digit(modpca$rotation[,3],axes=FALSE)
show_digit(modpca$rotation[,4],axes=FALSE)
par(mfrow=c(1,1))
```

Since each observation is made up of a combination of these Principle Components, we would expect the zeros to have high PC1 values, which we see is true by this plot below, with PC1 on the x-axis and PC2 on the y-axis. However, most of the classes are hard to seperate.

```{r, PCA plot}
#ggplot2::ggplot(data=as.data.frame(modpca$x[,1:2]), ggplot2::aes(x=PC1, y=PC2, color=trainset$y)) +
#        ggplot2::geom_point(size=2) + RRhelpr::theme_RR() + scale_color_brewer(type="qual", palette="Set3") +
#        ggplot2::labs(x = paste("PC1 (", scales::percent(prop.pca[1]), " explained var.)", sep=""),
#                      y = paste("PC2 (", scales::percent(prop.pca[2]), " explained var.)", sep=""))


plot_ly(as.data.frame(modpca$x), x=~PC1, y=~PC2, z=~PC3, color = as.factor(trainset$y), type = 'scatter3d')
```

Similarly, this is the plot that the above python code generated:

![](Images/python_PCA.png)



## LDA

### Theory
### Use Cases
### Examples


# Support Vector Machines

### Theory

A support vector machine it a binary classifier that attempts to maximize the seperation between groups. It fits a hyperplane between the two classes, creating and maximizing a gap between them. This hyperplane is a linear seperator, but can be made nonlinear via kernels and modifying dimensionality. 

![](Images/SVM-illustration.jpg)

In its simplest form, the process is as follows:


Let $H_1$ and $H_2$ be the closest points in different classes, $y$ be either 1 or -1 depending on the class of the point, and the hyperplane be defined as $w*x + b = 0$ so that points on different sides of the plane will have either 1 or -1. 

Then $d(H_1,H_2) = \frac{2*(w*x + b)}{w} = \frac{2}{w}$, and the goal is to minimize $w$ to maximize seperation. 

From the definitions above, we know that $w*x + b \geq 1$ when $y_1 = 1$ and that $w*x + b \leq -1$ when $y_1 = -1$, which can be simplified to $y_1 (w*x) \geq 1$. 

This can be solved with a [Lagrangian multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier) and has only one global maximum, which is the desired hyperplane.


### Use Cases

SVM is able to capture complex hyperplane boundries using non-linear kernels, but can be computationally intensive. It is most useful where boundries between classes are complex or not quite visible, as the hyperplanes can be calcualted to find the best boundaries, and ought to be more robust than linear methods when extrapolating to new data.

### Examples

```{r}
library('e1071')
model <- svm(y~., trainset, scale=F, kernel = "polynomial")
res <- predict(model, newdata=testset)

RRhelpr::printClassificationTable(testset$y,res,"SVM")
```

```{python}
import feather, os
import pandas as pd
from sklearn import svm

path = os.getcwd()
trainpath = os.path.join(path, "trainset.feather")
testpath = os.path.join(path, "testset.feather")

#data intake and prep
train = feather.read_dataframe(trainpath)
lbls = train['y']
imgs = train.drop('y', 1)
test = feather.read_dataframe(testpath)
lbls_test = test['y']
imgs_test = test.drop('y', 1)

#initialize and train model
svm = svm.SVC(kernel='poly')
svm.fit(imgs, lbls)
svm.score(imgs, lbls)

#predict
predictions = svm.predict(imgs_test)
df_confusion = pd.crosstab(lbls_test, predictions, margins=True)
print(df_confusion)
```

# KNN

# Bayesian

# Decision Trees

### Theory

Typically, a decision tree algorithm seeks to split the data as best as possible at each step (leaf), maximizing the difference between the two resulting leaves until no more useful splits can be found. For classification trees, these best splits are usually calculated by one of two indeces: Gini Impurity or Information Gain.

#### Gini Impurity 
Gini Impurity measures the innaccuracy of a split for a given class $i$, and ought to be minimized:
Supposing we have $J$ classes, $i \in {1,2,3,...,J}$ and $g_i$ is the fraction of observations that are labeled with class $i$.
$$ I_G (f) = \sum_{i=1}^{J} f_i (1-f_i) = 1 - \sum_{i=1}^{J} f_i^2 = \sum_{i \neq k} f_i f_k $$

#### Information Gain
Information gain compares the distribution of observations in the parent leaf to the distributions in the child leaves using the following formula:

$$ IG(T,x) =  H(T) - H(T|x) \: \text{where} \: H(s) = - \sum_{i=1}^{J} p_i log_2 p_i \: \text{and} \: p_i = \text{percentage of class} \: i \: \text{in a leaf} $$

This index ought to be maximized.

Although these are seperate measures, they almost always produce the same results, so either or both could be tried in most cases.
# Neural Networks

# HMM

# Ensemble Techniques

## Random Forest
## Bagging & Boosting

# Unsupervised Learning